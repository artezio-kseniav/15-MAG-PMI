{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'Ksenia Voronaya'\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_data = fetch_20newsgroups(categories=['rec.motorcycles', 'rec.autos', \n",
    "                                             'rec.sport.hockey', 'soc.religion.christian', \n",
    "                                             'alt.atheism', 'sci.electronics'], subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of topics (6 categories):\n",
      "['alt.atheism', 'rec.autos', 'rec.motorcycles', 'rec.sport.hockey', 'sci.electronics', 'soc.religion.christian']\n",
      "Size of corpus is 5765 documents\n"
     ]
    }
   ],
   "source": [
    "print(\"List of topics ({} categories):\".format(len(list(corpus_data.target_names))))\n",
    "print(list(corpus_data.target_names))\n",
    "print(\"Size of corpus is {} documents\".format(len(corpus_data.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenized_data = [nltk.word_tokenize(text) for text in corpus_data.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of tokenized corpus is 5765 documents\n"
     ]
    }
   ],
   "source": [
    "corpus_texts = []\n",
    "for text in tokenized_data:\n",
    "    lemms = []\n",
    "    for word in text:\n",
    "        if word not in string.punctuation:\n",
    "            lemms.append(word.lower())\n",
    "    corpus_texts.append(lemms)\n",
    "    \n",
    "print(\"Size of tokenized corpus is {} documents\".format(len(corpus_texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train our model by using Word2Vec\n",
    "import gensim\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(corpus_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Similarity between words motorcycle & car *******\n",
      "0.712350297086\n",
      "******* Similarity between words phone & gadget *******\n",
      "0.349898904959\n",
      "******* Similarity between words christianity & buddhism *******\n",
      "0.421998909873\n",
      "******* Similarity between words hockey & football *******\n",
      "0.557867952211\n",
      "******* Similarity between words computer & hockey *******\n",
      "0.259170065077\n",
      "******* Similarity between words motorcycle & computer *******\n",
      "0.0926919409609\n",
      "******* Similarity between words motorcycle & christianity *******\n",
      "0.0593047661766\n"
     ]
    }
   ],
   "source": [
    "# similarity of words\n",
    "words = [('motorcycle', 'car'), ('phone', 'gadget'), ('christianity', 'buddhism'), \n",
    "         ('hockey', 'football'), ('computer', 'hockey'), \n",
    "         ('motorcycle', 'computer'), ('motorcycle', 'christianity')]\n",
    "\n",
    "for word_pair in words:\n",
    "    print(\"******* Similarity between words {} & {} *******\".format(word_pair[0], word_pair[1]))\n",
    "    print(w2v_model.similarity(word_pair[0], word_pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Word which doesn't match the list [motorcycle car buddhism] *******\n",
      "buddhism\n",
      "******* Word which doesn't match the list [hockey football computer] *******\n",
      "computer\n",
      "******* Word which doesn't match the list [computer phone gadget] *******\n",
      "gadget\n"
     ]
    }
   ],
   "source": [
    "# word doesn't match the list\n",
    "words_list = [\"motorcycle car buddhism\", \"hockey football computer\", \"computer phone gadget\"]\n",
    "\n",
    "for w in words_list:\n",
    "    print(\"******* Word which doesn't match the list [{}] *******\".format(w))\n",
    "    print(w2v_model.doesnt_match(w.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Add up the two words 'king & queen' *******\n",
      "[(u'saint', 0.9126960039138794), (u'lamb', 0.8956121206283569), (u'abraham', 0.8902089595794678), (u'gabriel', 0.8830323815345764), (u'hippo', 0.8762622475624084), (u'actor', 0.8716279864311218), (u'3:15', 0.8711950182914734), (u'tibi', 0.8706649541854858), (u'collapsed', 0.868289053440094), (u'jacob', 0.8666414022445679)]\n",
      "******* Add up the two words 'city & russia' *******\n",
      "[(u'slovakia', 0.9462153315544128), (u'sweep', 0.9222856163978577), (u'district', 0.9178619384765625), (u'standings', 0.912844181060791), (u'annual', 0.9102997779846191), (u'minnesota', 0.9098118543624878), (u'friday', 0.909060001373291), (u'alberta', 0.9087628722190857), (u'gulls', 0.908035397529602), (u'dallas', 0.9067976474761963)]\n",
      "******* Add up the two words 'apple & computer' *******\n",
      "[(u'engineering', 0.9446059465408325), (u'computing', 0.9366177320480347), (u'services', 0.9297630190849304), (u'dept', 0.9272782802581787), (u'lab', 0.921724796295166), (u'department', 0.9203120470046997), (u'georgia', 0.9175235629081726), (u'nc', 0.9174311757087708), (u'universities', 0.914453387260437), (u'carnegie', 0.912139356136322)]\n"
     ]
    }
   ],
   "source": [
    "# addition and subtraction of words\n",
    "words_list = [['king', 'queen', 'woman'], ['city', 'russia', 'moscow'], \n",
    "              ['apple', 'computer', 'jobs']]\n",
    "\n",
    "for words in words_list:\n",
    "    print(\"******* Add up the two words '{} & {}' *******\".format(words[0], words[1]))\n",
    "    print(w2v_model.most_similar(positive=[words[0], words[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Add up and subtraction of words 'king & queen & woman' *******\n",
      "[(u'westfield', 0.8731837272644043), (u'stafford', 0.8714724779129028), (u'scherrer', 0.8682454228401184), (u'vax2.winona.msus.edu', 0.8664813041687012), (u'jpc', 0.8601957559585571), (u'====================================================', 0.8589756488800049), (u'philabs.philips.com', 0.8550039529800415), (u'kempmp', 0.8524545431137085), (u'franjione', 0.8481173515319824), (u'arnold', 0.8415936231613159)]\n",
      "******* Add up and subtraction of words 'city & russia & moscow' *******\n",
      "[(u'april', 0.8264734745025635), (u'division', 0.8034517765045166), (u'1993', 0.7940775156021118), (u'minnesota', 0.7915362119674683), (u'sweden', 0.7791882753372192), (u'standings', 0.7762460708618164), (u'finland', 0.7669036984443665), (u'friday', 0.766826868057251), (u'chicago', 0.7652980089187622), (u'alberta', 0.765078067779541)]\n",
      "******* Add up and subtraction of words 'apple & computer & jobs' *******\n",
      "[(u'engineering', 0.8233916759490967), (u'ibm', 0.8131964206695557), (u'dept', 0.807506799697876), (u'relva.rchland.ibm.com', 0.8060644268989563), (u'department', 0.8012591600418091), (u'uk', 0.7872180938720703), (u'technology', 0.7854435443878174), (u'institute', 0.78508460521698), (u'research', 0.7807159423828125), (u'services', 0.7723796963691711)]\n"
     ]
    }
   ],
   "source": [
    "for words in words_list:\n",
    "    print(\"******* Add up and subtraction of words '{} & {} & {}' *******\".format(words[0], words[1], words[2]))\n",
    "    print(w2v_model.most_similar(positive=[words[0], words[1]], negative=[words[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
