{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html as html\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from cookielib import CookieJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 499/499 [02:21<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15: 499\n",
      "24"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [02:36<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "24: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cookie enable\n",
    "cj = CookieJar()\n",
    "opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n",
    "\n",
    "def getPageText( pagelink ):\n",
    "        text = []\n",
    "        page = opener.open('http://izvestia.ru' + pagelink).read()\n",
    "        #print 'http://izvestia.ru' + pagelink\n",
    "        response = BeautifulSoup( page , \"html.parser\", parse_only=SoupStrainer(\"div\", {'class':'text_block'}) )\n",
    "        for a in response:\n",
    "            text.append(a.text)\n",
    "        return '\\n'.join(text)\n",
    "\n",
    "def grabNewForGroup( group, pageLimit ):\n",
    "    sports_pages = []\n",
    "    for pageNumber in range(1,pageLimit+1):\n",
    "        url = '''http://izvestia.ru/archive/{}?type=1&p={}'''.format(group,pageNumber)\n",
    "        p = opener.open(url)\n",
    "        page = p.read()\n",
    "        #print page\n",
    "        response = BeautifulSoup( page , \"html.parser\", parse_only=SoupStrainer(\"div\",{'class':'items_list_text'}) )\n",
    "\n",
    "        for a in BeautifulSoup( str(response) , \"html.parser\", parse_only=SoupStrainer(\"a\") ):\n",
    "            if 'href' in a.attrs:\n",
    "                if re.search(\"\\/news\\/[0-9]+\", a['href']):\n",
    "                    sports_pages.append(a['href'])\n",
    "\n",
    "\n",
    "    sports_pages = set(sports_pages)                \n",
    "    sport_texts = []\n",
    "    print group\n",
    "    from tqdm import tqdm\n",
    "    for link in tqdm(set(sports_pages)): \n",
    "        sport_texts.append(getPageText(link))\n",
    "    print group + \": \" + str(len(sports_pages))\n",
    "    return sport_texts\n",
    "\n",
    "    \n",
    "groups = ['15', # politics\n",
    "          '24']  # science\n",
    "         \n",
    "group_texts = dict()\n",
    "\n",
    "for g in groups:\n",
    "    group_texts[g] = grabNewForGroup(g, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_corpus = {}\n",
    "news_corpus['politics'] = group_texts['15']\n",
    "news_corpus['science'] = group_texts['24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save\n",
    "import pickle\n",
    "with open('dump.txt', 'w') as f:\n",
    "    pickle.dump(news_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load\n",
    "with open('dump.txt', 'r') as f:\n",
    "    news_corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print len(news_corpus['politics'])\n",
    "print len(news_corpus['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ngrams(input_file):\n",
    "    input_f = io.open(input_file, \"r\", encoding='utf8')\n",
    "    n_dict = {}\n",
    "    for line in input_f:\n",
    "        line = line.replace('\\n', '').split('\\t')\n",
    "        n_dict[tuple(line[0:])] = int(line[0])\n",
    "    input_f.close()\n",
    "    return n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2grams count:  6750525\n",
      "3grams count:  4655170\n"
     ]
    }
   ],
   "source": [
    "bigrams = load_ngrams('2grams-3.txt')\n",
    "print '2grams count: ', len(bigrams.keys())\n",
    "\n",
    "threegrams = load_ngrams('3grams-3.txt')\n",
    "print '3grams count: ', len(threegrams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = defaultdict(list)\n",
    "for key in news_corpus.keys():\n",
    "    for text in news_corpus[key]:\n",
    "        text_str = ''.join(text)\n",
    "        line = nltk.word_tokenize(text_str)\n",
    "        line = [word.encode(\"utf-8\").decode(\"utf-8\").lower() for word in line if word not in string.punctuation\n",
    "                        and word not in [u'\\u2012', u'\\u2013', u'\\u2014', u'\\u2015', u'``', u\"''\"]]\n",
    "        tokenized_corpus[key] += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('politics', 281647)\n",
      "('science', 270218)\n"
     ]
    }
   ],
   "source": [
    "for key in tokenized_corpus.keys():\n",
    "    print(key, len(tokenized_corpus[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_perplexity(category, n3_gram, n2_gram):\n",
    "    sum_probability = 0\n",
    "    unknown_words = 0\n",
    "    min_probability = 1\n",
    "    sum_log = 0\n",
    "    \n",
    "    current_probability= {}\n",
    "    \n",
    "    for i in range(2, len(category)):\n",
    "        try:\n",
    "            current_probability[i] = (n3_gram[(category[i-2], category[i-1], category[i])] + 0.0) \n",
    "            / n2_gram[(category[i-2],category[i-1])]\n",
    "            sum_probability += current_probability\n",
    "            if current_probability[i] < min_probability:\n",
    "                min_probability = current_probability[i]\n",
    "        except KeyError:\n",
    "            unknown_words +=1\n",
    "    print (unknown_words, len(category), sum_probability)\n",
    "    known_words = len(category) - unknown_words\n",
    "    min_probability /= 10.0\n",
    "    probability_to_substract = (unknown_words * min_probability + 0.0) / known_words\n",
    "    for i in range(2, len(category)):\n",
    "        try:\n",
    "            sum_log += math.log(current_probability[i] - probability_to_substract, 2)\n",
    "        except KeyError:\n",
    "            sum_log += math.log(min_probability, 2)\n",
    "\n",
    "    return sum_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281645, 281647, 0)\n",
      "politics  perplexity:  9.99983649264\n",
      "(270216, 270218, 0)\n",
      "science  perplexity:  9.99982957709\n"
     ]
    }
   ],
   "source": [
    "for key in tokenized_corpus.keys():\n",
    "    perplexity = find_perplexity(tokenized_corpus[key], bigrams, threegrams)\n",
    "    perplexity = 2**(-perplexity / len(tokenized_corpus[key]))\n",
    "    print key,' perplexity: ', perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
