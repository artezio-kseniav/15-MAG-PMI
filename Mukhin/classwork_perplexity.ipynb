{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html as html\n",
    "import re\n",
    "import time\n",
    "import io\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['politics', 'sport']\n",
    "news_url = {'http://izvestia.ru/archive/15?type=1&p=':'politics',  'http://izvestia.ru/archive/21?type=1&p=':'sport'}\n",
    "basic_url = 'http://izvestia.ru'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем ссылки на новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_urls = defaultdict(list)\n",
    "for url in news_url:\n",
    "    for i in range(1,150):\n",
    "        news = url + str(i)\n",
    "        news_html = html.parse(news)\n",
    "        found_urls = news_html.getroot().xpath('//div[@class=\"items_list_text\"]/h3/a')\n",
    "        for page_url in found_urls:\n",
    "            href = page_url.get('href')\n",
    "            news_urls[news_url[url]].append(basic_url + href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463\n",
      "1476\n"
     ]
    }
   ],
   "source": [
    "#urls count\n",
    "print(len(news_urls['politics']))\n",
    "print(len(news_urls['sport']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим тексты новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "news_corpus = defaultdict(list)\n",
    "for topic in topics:\n",
    "    count = 0\n",
    "    for news_page in  news_urls[topic]:\n",
    "        page_html = html.parse(news_page)\n",
    "        text = page_html.getroot().xpath('//div[@class=\"text_block\"]/p/text()')\n",
    "        head = page_html.getroot().xpath('//h1[@itemprop=\"headline\"]/text()')\n",
    "        if not text:\n",
    "            continue\n",
    "        news_text = ''.join(head) + ''.join(text)\n",
    "        news_corpus[topic].append(news_text)\n",
    "        count += 1\n",
    "        print(\"Topic: %s Parsing progress: %d out of %d \" % (topic, count, len(news_urls[topic])), end = '\\r' )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411\n",
      "1079\n"
     ]
    }
   ],
   "source": [
    "print(len(news_corpus['politics']))\n",
    "print(len(news_corpus['sport']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем биграммы и триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ngrams(input_file):\n",
    "    input_f = io.open(input_file, \"r\", encoding='utf8')\n",
    "    n_dict = {}\n",
    "    for line in input_f:\n",
    "        line = line.replace('\\n', '').split('\\t')\n",
    "        n_dict[tuple(line[0:])] = int(line[0])\n",
    "    input_f.close()\n",
    "    return n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2grams count:  6750525\n",
      "3grams count:  4655170\n"
     ]
    }
   ],
   "source": [
    "bigrams = load_ngrams('2grams-3.txt')\n",
    "print('2grams count: ', len(bigrams.keys()))\n",
    "\n",
    "threegrams = load_ngrams('3grams-3.txt')\n",
    "print('3grams count: ', len(threegrams.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_corpus = defaultdict(list)\n",
    "for key in news_corpus.keys():\n",
    "    for text in news_corpus[key]:\n",
    "        text_str = ''.join(text)\n",
    "        line = nltk.word_tokenize(text_str)\n",
    "        line = [word.encode(\"utf-8\").decode(\"utf-8\").lower() for word in line if word not in string.punctuation\n",
    "                        and word not in [u'\\u2012', u'\\u2013', u'\\u2014', u'\\u2015', u'``', u\"''\"]]\n",
    "        tokenized_corpus[key] += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics 740534\n",
      "sport 463327\n"
     ]
    }
   ],
   "source": [
    "for key in tokenized_corpus.keys():\n",
    "    print(key, len(tokenized_corpus[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим перплексию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_perplexity(category, n3_gram, n2_gram):\n",
    "    sum_probability = 0\n",
    "    unknown_words = 0\n",
    "    min_probability = 1\n",
    "    sum_log = 0\n",
    "    \n",
    "    current_probability= {}\n",
    "    \n",
    "    for i in range(2, len(category)):\n",
    "        try:\n",
    "            current_probability[i] = (n3_gram[(category[i-2], category[i-1], category[i])] + 0.0) \n",
    "            / n2_gram[(category[i-2],category[i-1])]\n",
    "            sum_probability += current_probability\n",
    "            if current_probability[i] < min_probability:\n",
    "                min_probability = current_probability[i]\n",
    "        except KeyError:\n",
    "            unknown_words +=1\n",
    "    print (unknown_words, len(category), sum_probability)\n",
    "    known_words = len(category) - unknown_words\n",
    "    min_probability /= 10.0\n",
    "    probability_to_substract = (unknown_words * min_probability + 0.0) / known_words\n",
    "    for i in range(2, len(category)):\n",
    "        try:\n",
    "            sum_log += math.log(current_probability[i] - probability_to_substract, 2)\n",
    "        except KeyError:\n",
    "            sum_log += math.log(min_probability, 2)\n",
    "\n",
    "    return sum_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740532 740534 0\n",
      "politics  perplexity:  9.999937813028556 \n",
      "\n",
      "463325 463327 0\n",
      "sport  perplexity:  9.999900606772984 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in tokenized_corpus.keys():\n",
    "    perplexity = find_perplexity(tokenized_corpus[key], bigrams, threegrams)\n",
    "    perplexity = 2**(-perplexity / len(tokenized_corpus[key]))\n",
    "    print(key,' perplexity: ', perplexity, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вероятностностная модель корпуса категории новостей \"Спорт\" обладает лучшей предсказательной способностью, чем модель корпуса категории \"Политика\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
