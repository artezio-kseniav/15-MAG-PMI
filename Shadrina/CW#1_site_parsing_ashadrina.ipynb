{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16.05 Classwork №1: parse news site to create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxml.html as html\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем парсить The Guardian. Они предоставляют бесплатный API для разработчиков на следующих условиях: 12 вызовов в секунду, 5 000 за день, доступ к полному тексту статьи открыт. Возвращаемый формат - JSON.\n",
    "Для получения ключа нужно пройти регистрацию по ссылке: http://open-platform.theguardian.com/access/ . В ответ на почту сразу высылается api-key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Информация о корпусе:\n",
    "<br> - Время: с 1 января 2016 до 28 мая 2016\n",
    "<br> - Категории: 5 (новости Великобритании, мировые новости, спорт, культура, мода)\n",
    "<br> - Выдача на 1 категорию: 200 статей\n",
    "<br> - Поля: ссылка, заголовок, полный текст, категория, тэги.\n",
    "При желании можно так же вытащить дату публикации на сайте и в печати, информацию об авторе, тип (статья или блог), изменялся ли текст, если к статье есть фото, то имя фотографа. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "site_url = 'http://content.guardianapis.com/search?section='\n",
    "api_key = '&from-date=2016-01-01&page-size=200&show-tags=all&show-blocks=all&api-key=cc63bd2e-e220-459f-82c1-2e9943816bf9'\n",
    "categories = ['uk-news', 'world', 'sport', 'culture', 'fashion']\n",
    "\n",
    "corpus = {} #{section: category, articles: {url: url, attrs: {text: text, title :title,  tags: [list_of_tags]} } }\n",
    "for category in categories: \n",
    "    r = requests.get(site_url+category+api_key)\n",
    "    content_string = r.content.decode(\"utf-8\")\n",
    "    content_json = json.loads(content_string)\n",
    "    attrs = {}\n",
    "    articles = {}\n",
    "    for item in content_json['response']['results']:\n",
    "        attrs['title'] = item['webTitle']\n",
    "        attrs['section'] = item['sectionId']\n",
    "        tags = []\n",
    "        for i in range(len( item['tags'])):\n",
    "            #print ('tags: ', item['tags'][i]['id'])\n",
    "            tags.append(item['tags'][i]['id'])\n",
    "        #print (item['sectionId'], ': ', item['webTitle'], item['webUrl'])    \n",
    "        #print (item['blocks']['body'][0]['bodyTextSummary'])\n",
    "        attrs['tags'] = tags\n",
    "        attrs['text'] = item['blocks']['body'][0]['bodyTextSummary']     \n",
    "        articles[item['webUrl']] = attrs  \n",
    "    corpus[category] = articles   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world 200\n",
      "uk-news 200\n",
      "sport 200\n",
      "culture 200\n",
      "fashion 200\n"
     ]
    }
   ],
   "source": [
    "for k,v in corpus.items():\n",
    "    print (k, len(v.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
