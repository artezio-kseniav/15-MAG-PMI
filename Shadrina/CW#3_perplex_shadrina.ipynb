{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 01.06 Classwork №3: русскоязычный корпус и перплексия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем парсить Медузу, 2 рубрики: новости и шапито"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "import json\n",
    "import requests\n",
    "\n",
    "site_url = 'https://meduza.io/api/v3/search?chrono='\n",
    "api_key='&page='\n",
    "api_key_2='&per_page=20&locale=ru'\n",
    "categories = ['news', 'shapito']\n",
    "\n",
    "corpus = {} #{category:articles[]}\n",
    "for category in categories: \n",
    "    articles = []\n",
    "    for page in range(0,50):    \n",
    "        #print (site_url+category+api_key+str(page)+api_key_2)\n",
    "        r = requests.get(site_url+category+api_key+str(page)+api_key_2)\n",
    "        content_string = r.content.decode(\"utf-8\")\n",
    "        content_json = json.loads(content_string)\n",
    "        for item in content_json['documents']:\n",
    "            full_text_url='https://meduza.io/api/v3/'+content_json['documents'][item]['url']\n",
    "            r2 = requests.get(full_text_url)\n",
    "            text_string = r2.content.decode(\"utf-8\")  \n",
    "            full_text_json = json.loads(text_string)\n",
    "            text = full_text_json['root']['content']['body']\n",
    "            document = lxml.html.document_fromstring(text)\n",
    "            cleantext = document.text_content()\n",
    "            articles.append([cleantext])\n",
    "    corpus[category] = articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapito 1000\n",
      "news 1000\n"
     ]
    }
   ],
   "source": [
    "for k,v in corpus.items():\n",
    "    print (k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus['news']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Триграммы из НКРЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_ngrams(in_file):\n",
    "    input_f = open(in_file, \"r\", encoding=\"utf8\")\n",
    "    n_dict = {}\n",
    "    for line in input_f:\n",
    "        line = line.replace('\\n','').split('\\t')\n",
    "        n_dict[tuple(line[0:])] = int(line[0])\n",
    "    input_f.close()\n",
    "    return n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6750525\n"
     ]
    }
   ],
   "source": [
    "n2_dict = load_ngrams(\"2grams-3.txt\")\n",
    "print (len(n2_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4655170\n"
     ]
    }
   ],
   "source": [
    "n3_dict = load_ngrams(\"3grams-3.txt\")\n",
    "print (len(n3_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перплексия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_perplexity(category, n3_dict, n2_dict):\n",
    "    sum_probability = 0\n",
    "    unknown_words = 0\n",
    "    min_probability = 1\n",
    "    sum_log = 0\n",
    "\n",
    "    for i in range(2, len(category)):\n",
    "        try:\n",
    "            current_probability = (n3_dict[(category[i-2], category[i-1], category[i])] + 0.0) \n",
    "            / n2_dict[(category[i-2],category[i-1])]\n",
    "            sum_probability += current_probability\n",
    "            if current_probability < min_probability:\n",
    "                min_probability = current_probability\n",
    "        except KeyError:\n",
    "            unknown_words +=1\n",
    "    print (unknown_words, len(category), sum_probability)\n",
    "    known_words = len(category) - unknown_words\n",
    "    min_probability /= 10\n",
    "    probability_to_substract = (unknown_words * min_probability + 0.0) / known_words\n",
    "    for i in range(2, len(category)):\n",
    "        try:\n",
    "            current_probability = (n3_dict[(category[i-2], category[i-1], category[i])] + 0.0) \n",
    "            / n2_dict[(category[i-2],category[i-1])]\n",
    "            sum_log += math.log(current_probability - probability_to_substract, 2)\n",
    "        except KeyError:\n",
    "            sum_log += math.log(min_probability, 2)\n",
    "\n",
    "    return sum_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "news_list = corpus['news']\n",
    "news = []\n",
    "for text in news_list:\n",
    "    text_str = ''.join(text)\n",
    "    line = nltk.word_tokenize(text_str)\n",
    "    line = [word.encode(\"utf-8\").decode(\"utf-8\").lower() for word in line if word not in string.punctuation\n",
    "                    and word not in [u'\\u2012', u'\\u2013', u'\\u2014', u'\\u2015', u'``', u\"''\"]]\n",
    "    news += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288530 288532 0\n",
      "Perplexity for news =  9.999840394239554\n"
     ]
    }
   ],
   "source": [
    "sum_log_2 = compute_perplexity(news, n3_dict, n2_dict)\n",
    "print(\"Perplexity for news = \",2**(-sum_log_2 / len(news)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "shapito_list = corpus['shapito']\n",
    "shapito = []\n",
    "for text in shapito_list:\n",
    "    text_str = ''.join(text)\n",
    "    line = nltk.word_tokenize(text_str)\n",
    "    line = [word.encode(\"utf-8\").decode(\"utf-8\").lower() for word in line if word not in string.punctuation\n",
    "                    and word not in [u'\\u2012', u'\\u2013', u'\\u2014', u'\\u2015', u'``', u\"''\"]]\n",
    "    shapito += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179870 179872 0\n",
      "Perplexity for shapito =  40.18695746319039\n"
     ]
    }
   ],
   "source": [
    "sum_log_1 = compute_perplexity(shapito, n3_dict, n2_dict)\n",
    "print(\"Perplexity for shapito = \",2**(-sum_log_2 / len(shapito)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
