{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim.models import Word2Vec\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totics: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Totics num: 20\n",
      "Total files: 18846\n"
     ]
    }
   ],
   "source": [
    "texts = fetch_20newsgroups(subset='all')\n",
    "print 'Totics:',texts.target_names\n",
    "print 'Totics num:', len(texts.target_names)\n",
    "print 'Total files:', len(texts.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_texts = []\n",
    "for text in texts.data:\n",
    "    tokenized_texts.append(nltk.word_tokenize(text))\n",
    "\n",
    "lemmas = []\n",
    "for text in tokenized_texts:\n",
    "    lemma = []\n",
    "    for word in text:\n",
    "        if word not in string.punctuation:\n",
    "            lemma.append(word.lower())\n",
    "    lemmas.append(lemma)\n",
    "\n",
    "word2Vec = Word2Vec(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#similarity func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Similarity for words data and information: 0.664886924496\n",
      "2. Similarity for words russia and usa: 0.309541167625\n",
      "3. Similarity for words pc and computer: 0.51949545241\n",
      "4. Similarity for words live and love: 0.506881360198\n",
      "5. Similarity for words bus and car: 0.464064694192\n",
      "6. Similarity for words phone and apple: 0.330979627706\n",
      "7. Similarity for words berry and apple: 0.0137398367646\n",
      "8. Similarity for words pen and dictionary: 0.282381295974\n"
     ]
    }
   ],
   "source": [
    "test_pairs = [('data', 'information'), ('russia', 'usa'), ('pc', 'computer'),\n",
    "            ('live', 'love'), ('bus', 'car'), ('phone', 'apple'),\n",
    "            ('berry', 'apple'), ('pen', 'dictionary')]\n",
    "\n",
    "for id,pair in enumerate(test_pairs):\n",
    "    print(\"{}. Similarity for words {} and {}: {}\".format(id+1, pair[0], pair[1], word2Vec.similarity(pair[0], pair[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#doesnt_match func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Excess word in [apple fruit usa] is fruit\n",
      "2. Excess word in [data information pen] is pen\n"
     ]
    }
   ],
   "source": [
    "test_pairs = [\"apple fruit usa\", \"data information pen\"]\n",
    "\n",
    "for id,pair in enumerate(test_pairs):\n",
    "    print(\"{}. Excess word in [{}] is {}\".format(id+1, pair, word2Vec.doesnt_match(pair.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#most_similar func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'tech', 0.6710723638534546),\n",
       " (u'company', 0.6660571098327637),\n",
       " (u'800', 0.6305512189865112),\n",
       " (u'gateway', 0.6247504949569702),\n",
       " (u'computers', 0.6237499713897705),\n",
       " (u'1-614-292-1479', 0.619231641292572),\n",
       " (u'modem', 0.6106947660446167),\n",
       " (u'bbs', 0.6075387001037598),\n",
       " (u'network', 0.6021898984909058),\n",
       " (u'voice', 0.5933206677436829)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2Vec.most_similar(positive=['phone', 'computer', 'printer'], negative = ['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
